{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-deegup\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-deegup\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\datasets\\imdb.py:49: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n",
      "C:\\Users\\t-deegup\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\datasets\\reuters.py:47: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n"
     ]
    }
   ],
   "source": [
    "(train_data_a, train_labels_a), (test_data_a, test_labels_a) = imdb.load_data(nb_words=5000)\n",
    "(train_data_b, train_labels_b), (test_data_b, test_labels_b) = reuters.load_data(nb_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "max_text_length = 100\n",
    "\n",
    "# vectorized training data\n",
    "x_train_a = sequence.pad_sequences(train_data_a, maxlen=max_text_length)[:1000]\n",
    "x_train_b = sequence.pad_sequences(train_data_b, maxlen=max_text_length)[:1000]\n",
    "x_test_a = sequence.pad_sequences(test_data_a, maxlen=max_text_length)[:1000]\n",
    "x_test_b = sequence.pad_sequences(test_data_b, maxlen=max_text_length)[:1000]\n",
    "\n",
    "y_train_a = to_categorical(train_labels_a)[:1000]\n",
    "y_test_a = to_categorical(test_labels_a)[:1000]\n",
    "y_train_b = to_categorical(train_labels_b)[:1000]\n",
    "y_test_b = to_categorical(test_labels_b)[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "vocabulary_size = 5000\n",
    "embedding_dim=50\n",
    "shared_lstm = LSTM(64)\n",
    "\n",
    "input1 = Input(shape=(max_text_length,), dtype='int32')\n",
    "input2 = Input(shape=(max_text_length,), dtype='int32')\n",
    "\n",
    "embedding1 = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=max_text_length)(input1)\n",
    "embedding2 = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=max_text_length)(input2)\n",
    "lstm_output_1=shared_lstm(embedding1)\n",
    "lstm_output_2=shared_lstm(embedding2)\n",
    "hidden_output_1 = Dense(100, activation='relu')(lstm_output_1)\n",
    "hidden_output_2 = Dense(100, activation='relu')(lstm_output_2)\n",
    "output1=Dense(2, activation='softmax')(hidden_output_1)\n",
    "output2=Dense(46, activation='softmax')(hidden_output_2)\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     loss1=softmax(y_true,y_pred)\n",
    "#     loss2=center_loss(y_true,fc)\n",
    "#     return loss1+lambda*loss2\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=[output1, output2])\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=['categorical_crossentropy', 'categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-deegup\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 4.4499 - dense_7_loss: 0.6881 - dense_8_loss: 3.7619 - dense_7_acc: 0.6660 - dense_8_acc: 0.3540 - val_loss: 4.3787 - val_dense_7_loss: 0.6897 - val_dense_8_loss: 3.6889 - val_dense_7_acc: 0.5690 - val_dense_8_acc: 0.3640\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 4.2990 - dense_7_loss: 0.6803 - dense_8_loss: 3.6186 - dense_7_acc: 0.6370 - dense_8_acc: 0.3540 - val_loss: 3.6113 - val_dense_7_loss: 0.6979 - val_dense_8_loss: 2.9135 - val_dense_7_acc: 0.4760 - val_dense_8_acc: 0.3640\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.6324 - dense_7_loss: 0.8501 - dense_8_loss: 2.7823 - dense_7_acc: 0.4950 - dense_8_acc: 0.3540 - val_loss: 3.2515 - val_dense_7_loss: 0.6881 - val_dense_8_loss: 2.5635 - val_dense_7_acc: 0.6060 - val_dense_8_acc: 0.3640\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.1857 - dense_7_loss: 0.6794 - dense_8_loss: 2.5063 - dense_7_acc: 0.6720 - dense_8_acc: 0.3540 - val_loss: 3.1999 - val_dense_7_loss: 0.6874 - val_dense_8_loss: 2.5124 - val_dense_7_acc: 0.5250 - val_dense_8_acc: 0.3640\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.1235 - dense_7_loss: 0.6624 - dense_8_loss: 2.4611 - dense_7_acc: 0.6740 - dense_8_acc: 0.3540 - val_loss: 3.1510 - val_dense_7_loss: 0.6689 - val_dense_8_loss: 2.4821 - val_dense_7_acc: 0.6660 - val_dense_8_acc: 0.3640\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.0563 - dense_7_loss: 0.6260 - dense_8_loss: 2.4304 - dense_7_acc: 0.7760 - dense_8_acc: 0.3540 - val_loss: 3.1337 - val_dense_7_loss: 0.6793 - val_dense_8_loss: 2.4545 - val_dense_7_acc: 0.5570 - val_dense_8_acc: 0.3640\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.0302 - dense_7_loss: 0.6353 - dense_8_loss: 2.3949 - dense_7_acc: 0.7590 - dense_8_acc: 0.3540 - val_loss: 3.1022 - val_dense_7_loss: 0.6558 - val_dense_8_loss: 2.4464 - val_dense_7_acc: 0.5500 - val_dense_8_acc: 0.3640\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.9736 - dense_7_loss: 0.5899 - dense_8_loss: 2.3838 - dense_7_acc: 0.7740 - dense_8_acc: 0.3540 - val_loss: 3.0609 - val_dense_7_loss: 0.6317 - val_dense_8_loss: 2.4292 - val_dense_7_acc: 0.6720 - val_dense_8_acc: 0.3640\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.8827 - dense_7_loss: 0.5229 - dense_8_loss: 2.3598 - dense_7_acc: 0.8420 - dense_8_acc: 0.3540 - val_loss: 2.9936 - val_dense_7_loss: 0.6512 - val_dense_8_loss: 2.3424 - val_dense_7_acc: 0.6680 - val_dense_8_acc: 0.3640\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.7876 - dense_7_loss: 0.5380 - dense_8_loss: 2.2496 - dense_7_acc: 0.9180 - dense_8_acc: 0.3540 - val_loss: 3.0755 - val_dense_7_loss: 0.6295 - val_dense_8_loss: 2.4460 - val_dense_7_acc: 0.7140 - val_dense_8_acc: 0.3640\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_train_a, x_train_b],[y_train_a, y_train_b], nb_epoch=10, batch_size=512, validation_data=([x_test_a, x_test_b], [y_test_a, y_test_b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
